{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to build a Machine Learning model with Python (and no PhD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "1. What is ML?\n",
    "2. What is a tidy dataset?\n",
    "2. An overview of the data pipeline (with Python)\n",
    "3. Let's train a model\n",
    "\n",
    "Through all this tutorial, if you don't have the solution to one exercice, you can uncomment the line `# %load solutions/....` to get the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is ML?\n",
    "Machine Learning is a way to get computers to do what you want without **explicitely** programming them, by instead feeding them **examples** of what you want.\n",
    "\n",
    "1. Don't use ML if you can program the behaviour explicitely!\n",
    "2. Don't use ML if you don't have data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is ML?\n",
    "\n",
    "Machine Learning is mostly composed of two steps:\n",
    "1. Encoding your data in a high-dimensional vector space\n",
    "2. Learning, which is minimising your loss function in this vector space\n",
    "\n",
    "![Gradient descent](img/3d-gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tidy Datasets\n",
    "\n",
    "Before starting any Machine Learning project, you'll need data. And in addition, you will want your data to be in the form of a **tidy dataset**: \n",
    "* Each variable forms a column and contains values\n",
    "* Each observation forms a row\n",
    "* Each type of observational unit forms a table\n",
    "\n",
    "This form will enable you to perform easily data analysis, and then Machine Learning. This [blog article](http://www.jeannicholashould.com/tidy-data-in-python.html) is a must-read to understand tidy datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In practice\n",
    "\n",
    "1. Data wrangling: prepare your data, using Pandas (or the ETL of your choice) -> getting a tidy dataset out of your data\n",
    "2. Data preprocessing: Pandas or scikit-learn\n",
    "3. Model training and evaluation: scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The building block: NumPy\n",
    "\n",
    "NumPy is the fundamental library for scientific computing in Python. Here I will focus on the NumPy arrays, which are a way to efficiently store numerical matrices in memory, using a C-like representation. \n",
    "\n",
    "<img src=\"img/array_vs_list.png\" width=\"800\" />\n",
    "\n",
    "*Image from [this](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/) blog article*\n",
    "\n",
    "In addition to efficient storage of data, NumPy proposes a lot of so-called *universal functions* that are applied in batch (either element-wise to a whole array (map) or as a reduce operation axis-wise). It also defines vectorial and matrix operations. \n",
    "\n",
    "*If you really need a universal function that is not already implemented, you can always use [Numba](http://numba.pydata.org/numba-doc/dev/user/vectorize.html).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([[12, 117, 47], \n",
    "                [17, 178, 72],\n",
    "                [28, 179, 79]])\n",
    "np.power(arr, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step one: extract, transform, load (ETL) with Pandas\n",
    "\n",
    "NumPy arrays are great for performance, but can be tedious to handle in real world. First, your columns may not be all of the same types (numerical, categorical, datetime); then, it's error-prone to handle lines and columns by index rather than by name. \n",
    "\n",
    "Pandas DataFrame are built on top of NumPy arrays to alleviate these issues (and many more). It's my go-to library for ETL; when I'm done, I can convert my DataFrame to a numpy array for further computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=arr, columns=['Age', 'Size', 'Weight'], index=np.arange(1, 4))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step two: Machine Learning with scikit-learn\n",
    "\n",
    "Once I have a numpy array containing all my data, I can train machine learning algorithms on it. \n",
    "\n",
    "scikit-learn offers good implementations of typical machine learning estimators, and a unified API to bind them all. The documentation is also very well-written and will help you choose the right algorithm for your need. \n",
    "\n",
    "```\n",
    "my_classifier = Classifier(hyperparameters)\n",
    "my_classifier.fit(X_train, y_train)\n",
    "my_classifier.score(X_val, y_val)\n",
    "my_classifier.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python ML pipeline\n",
    "1. **Load** and tidy your data with pandas\n",
    "3. **Split** your data between a training dataset and a test dataset -> `sklearn.model_selection.train_test_split`\n",
    "4. **Clean** your data (missing values, categorical variables, etc) -> `df.fillna`, `pd.get_dummies`\n",
    "5. **Extract** your numerical data as numpy array -> `df.as_matrix`\n",
    "6. **Preprocess** data with scikit-learn, using a `sklearn.pipeline.Pipeline`\n",
    "7. **Learn** a scikit-learn model with a `sklearn.model_selection.cross_val_score` wrapper to evaluate how well your model would generalize\n",
    "8. Maybe **adjust** your model hyperparemeters -> `sklearn.model_selection.GridSearchCV`\n",
    "9. Learn your model on all your training data, then **evaluate** on your test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's learn a ML model!\n",
    "\n",
    "<img src=\"img/pokemon.jpg\" width=\"200\" />\n",
    "\n",
    "We want to learn to predict the outcome of pokemon battles. \n",
    "\n",
    "For that, we have the results of past battles in the file `pokemon-challenge/battles_train.csv`, and some characteristics of each Pokemon can be found in the file `pokemon-challenge/pokemon.csv`.\n",
    "\n",
    "Let's load the files with pandas and see what we have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "battles = pd.read_csv('pokemon-challenge/battles_train.csv')\n",
    "battles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pokemons = pd.read_csv('pokemon-challenge/pokemon.csv', index_col=0)\n",
    "pokemons.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pokemons.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pokemons.describe(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our data is split across two tables, and it is not at all *tidy*. \n",
    "\n",
    "We want to modify our data to have the following characteristics:\n",
    "* each Pokemon battle is represented by one line\n",
    "* each line contains all necessary information for prediction, ie the characteristics of both Pokemons and the outcome\n",
    "* the outcome will be stored in a column 'label',  with the value 1 if the first Pokemon won, and 0 if the second Pokemon won."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hints: \n",
    "\n",
    "You can compare two columns in pandas with: `df['A'] == df['B']`\n",
    "\n",
    "Pandas allows you to do all kind of joins, see the [documentation](https://pandas.pydata.org/pandas-docs/stable/merging.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/create_dataset.py\n",
    "# Creating the label column (don't forget we want it to be of type int)\n",
    "battles['label'] = \n",
    "# Joining the 3 tables into one\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we want to drop some irrelevant columns, such as indices. What might they be?\n",
    "\n",
    "We can first check the name of the columns of the dataframe with `df.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/columns.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/drop_columns.py\n",
    "# Drop unneccessary columns \n",
    "df_ml = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more thing, we said that we wanted to encode all of our data as a numerical vector, and we still have non-numerical data (types and legendary status). \n",
    "\n",
    "We can check the types of all columns with `df.dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/types.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The legendary status is a binary value, we can simply cast it as an int. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/cast_types.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the types, we will use the one-hot encoding, which is provided by the function [`pd.get_dummies`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html). \n",
    "\n",
    "A small subtetly here, each Pokemon can have up to 2 types, which we choose to encode without consideration for primary or secundary type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/one_hot_encoding.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's it, we're done with the data preparation! \n",
    "\n",
    "If you want to use the cleaned data for the rest of the workshop, you can load it from `pokemon-challenge/cleaned_train_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df_ml = pd.read_csv('pokemon-challenge/cleaned_train_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's train a decision tree model\n",
    "\n",
    "The first model we will learn is a decision tree. \n",
    "\n",
    "Decision trees create *if-then-else* rules in a top-down process, choosing at each level to split on the variable that optimize a criteria (minimize entropy for example). \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Decision trees : a toy example\n",
    "Let's try to predict if I should take ice-cream as dessert or not. \n",
    "\n",
    "I have written down a few variables that could influence my choice: whereas I'm on a diet or not, whereas it's summer or not, whereas the weather is sunny, whereas strawberries are available as an alternative, and whereas I ate pasta as main dish. The last column register whereas I had ice-cream as dessert.\n",
    "\n",
    "| Diet | Summer | Sunny | Strawberries | Pasta | Ice-cream |\n",
    "|------|--------|-------|--------------|-------|-----------|\n",
    "| 1    | 0      | 1     | 0            | 0     | 0         |\n",
    "| 0    | 1      | 1     | 0            | 0     | 1         |\n",
    "| 0    | 0      | 0     | 0            | 1     | 0         |\n",
    "| 0    | 1      | 0     | 1            | 0     | 0         |\n",
    "| 1    | 1      | 1     | 0            | 0     | 1         |\n",
    "| 0    | 0      | 1     | 0            | 0     | 0         |\n",
    "| 0    | 1      | 1     | 1            | 1     | 0         |\n",
    "| 0    | 1      | 1     | 0            | 0     | 1         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From this toy dataset, I could learn the following decision tree:\n",
    "\n",
    "![A toy decision tree](img/dt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision trees have the advantages to be **interpretable**, and to handle nicely multi-class classification and categorical variables. On the downside, they are quite instable and prone to **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to apply sklearn algorithms, we need to translate our pandas DataFrame to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = df_ml.drop('label', axis=1).as_matrix()\n",
    "y = df_ml['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since Decision Trees are so prone to overfitting, if we train our algorithm on the whole of the data, and then see how well it performs (that we call the training error), we will get a very optimistic estimate of how well the algorithm will perform in real life. \n",
    "\n",
    "One solution would be to split our training data into a train dataset and a validation dataset (see for example [`sklearn.model_selection.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/train_test_split.py\n",
    "\n",
    "X_train, X_val, y_train, y_val ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, you can instantiate a `DecisionTreeClassifier` with default hyperparameter values, and train it with the method `fit(X, y)`. You can evaluate how well it did with the method `score(X, y)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/decision_tree.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that with the default hyperparameters settings, the algorithm learns a tree that perfectly classify the training set, but the result is not impressive on new data (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another option, especially if you got little data, is to use cross-validation.\n",
    "\n",
    "At each iteration, we use 90% of the data to train a model, and the remaining 10% to evaluate how good the model is. And we repeat that 10 times, using a different 10% to evaluate each time. \n",
    "\n",
    "![Cross-validation](img/crossValidation.png)\n",
    "\n",
    "You can try it out with `sklearn.model_selection.cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/cross_val.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We said that Decision Trees have the huge advantage of being interpretable. One very nice feature is that they compute feature importances, a numerical value about how important a feature (= a variable) is to predict correctly the label. \n",
    "\n",
    "Let's learn again a tree, and see what the feature importances are (`dt.feature_importances_`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/feature_importance.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the feature importances, we can see that the most important variables are speed of both Pokemons; we can train a model with only those two variables and see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_speed = df_ml[['Speed', 'Speed_opponent']].as_matrix()\n",
    "cross_val_score(DecisionTreeClassifier(), X_speed, y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our accuracy is slightly worse, but we only use 2 variables instead of 50 ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ensemble methods: Random Forests\n",
    "\n",
    "Decision Trees are quick to train and quite easy to understand, but they have the downside of being very brittle and unstable. Changing only one sample in your training set can lead to the learning of a completely different tree. \n",
    "\n",
    "Here, I would like to introduce one of my favorite trick in Machine Learning: ensemble methods. \n",
    "\n",
    "The intuitive idea is that of the wisdom of crowd, or of expert committees. There are mathematical proofs that for some ensemble methods (like boosting), if you have a classifier that is slightly better than random, you can build an arbitrarily accurate ensemble from them. \n",
    "\n",
    "For Random Forests, the base classifier is a decision tree. In order to improve the robustness of prediction, we won't use only one tree, but rather average the prediction over a whole forest. \n",
    "\n",
    "How do we train this forest? For each tree, we draw with replacement a training set from the original training set. In addition, to ensure diversity, at each split, we choose the best from a random subset of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we don't have to worry about the implementation details for now, we can find an implementation in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/random_forest.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/compare.py\n",
    "\n",
    "rf_cv_scores = \n",
    "dt_cv_scores = \n",
    "\n",
    "print(f'Average accuracy for Decision Trees {dt_cv_scores.mean()} with a standard deviation of {dt_cv_scores.std()}')\n",
    "print(f'Average accuracy for Random Forests {rf_cv_scores.mean()} with a standard deviation of {rf_cv_scores.std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this dataset, we see that Random Forest does not perform better than Decision Tree, if we run them all with defaults parameters. \n",
    "\n",
    "We can also see that since scikit-learn provides a united API to interact with all ML estimators, it is tremendously easy to try out a new algorithm, and see what works best for you (in terms of accuracy, speed of execution or memory consumption). The documentation is also very useful to guide you on what usually works best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonus step: choose hyperparameters\n",
    "\n",
    "Until now, we have used all algorithms \"out of the box\", without choosing any hyperparameters. \n",
    "\n",
    "First, what are hyperparameters? We call hyperparameters the algorithm parameters which are not learned, but choosen by the user.\n",
    "\n",
    "For example, in the Random Forest algorithm, we can choose how many trees we want in our forest. The default is 10, but I can increase it to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=15), X, y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that the accuracy has improved. But how to know if 15 is the optimal value?\n",
    "\n",
    "scikit-learn has a function `sklearn.model_selection.GridSearchCV` to automatise the search over a grid of parameters. Here, we will only test the impact of the number of trees in Random Forest, varying the value from 5 to 55. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tested_values = {'n_estimators': list(range(5, 56, 5))}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), tested_values, cv=10)\n",
    "grid_search.fit(X, y)\n",
    "print(\"Best hyperparameter choice\", grid_search.best_params_)\n",
    "print(\"Averaged accuracy per run\", grid_search.cv_results_['mean_test_score'])\n",
    "print(\"Standard deviation per run\", grid_search.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonus step: encoding type in a different way\n",
    "\n",
    "Spoiler alert: categorical variables are often difficult to exploit. One-of-k encoding is clumsy when a variable can take many different values, and it can break links existing between categories. \n",
    "\n",
    "In this dataset, the type(s) of each Pokemon are categorical. If you know the game, you know that there exists a system of type (dis)advantage. For example, Fire is weak to Water, but strong against Ice.\n",
    "\n",
    "You can find the numerical values associated with each pair of types in the file type.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_types = pd.read_csv('pokemon-challenge/type.csv', index_col=0)\n",
    "df_types.columns.name = 'Attack type'\n",
    "df_types.index.name = 'Defensive pokemon type'\n",
    "df_types = df_types.replace(0, 0.01)\n",
    "df_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We replaced zeros (type immunity) with a small value, to avoid to mangage infinity (here, 100 will be infinity, which is much greater than all other values). \n",
    "\n",
    "Type advantages and disadvantages are combined when the Pokemon has more than one type. Also, it applies to the attack which is performed on the Pokemon, and each attack has only one type. We make the hypothesis when the attacker has more than one type that the type advantage will be the average. This is not necessarily true because, even though attack types are correlated to the Pokemon type, the Pokemon can also perform attacks of a different type (for example an Electric attack even if it's a Fire Pokemon). \n",
    "\n",
    "We compute the type advantage as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def type_advantage_offensive(t1, t2, to1, to2):\n",
    "    if not t2:\n",
    "        if not to2:\n",
    "            return df_types[t1][to1]\n",
    "        return (df_types[t1][to1] * df_types[t1][to2]) \n",
    "    if not to2:\n",
    "        return (df_types[t1][to1] + df_types[t2][to1]) / 2\n",
    "    return (df_types[t1][to1] * df_types[t2][to1] + df_types[t1][to2] * df_types[t2][to2]) / 2.  \n",
    "\n",
    "\n",
    "def type_advantage_defensive(t1, t2, to1, to2):\n",
    "    tao = type_advantage_offensive(to1, to2, t1, t2)\n",
    "    return  1. / type_advantage_offensive(to1, to2, t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_ml_type = df.drop(['First_pokemon', 'Second_pokemon', \n",
    "                      'Winner', 'Name', 'Name_opponent', \n",
    "                      'Generation', 'Generation_opponent'], axis=1)\n",
    "df_ml_type['Legendary'] = df_ml_type['Legendary'].astype(int)\n",
    "df_ml_type['Legendary_opponent'] = df_ml_type['Legendary_opponent'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "types_ =  df_ml_type[['Type 1', 'Type 2', 'Type 1_opponent', 'Type 2_opponent']].fillna(value='')\n",
    "df_ml_type['Type_advantage_offensive'] = types_.fillna('').apply(lambda x: type_advantage_offensive(*x), axis=1)\n",
    "df_ml_type['Type_advantage_defensive'] = types_.apply(lambda x: type_advantage_defensive(*x), axis=1)\n",
    "df_ml_type = df_ml_type.drop(['Type 1', 'Type 2', 'Type 1_opponent', 'Type 2_opponent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_ml_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can train all of our ML models as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_type = df_ml_type.drop('label', axis=1).as_matrix()\n",
    "y_type = df_ml_type['label'].as_matrix()\n",
    "cross_val_score(DecisionTreeClassifier(), X, y, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_type, y_type)\n",
    "pd.DataFrame(data=dt.feature_importances_, index=df_ml_type.columns[1:],\n",
    "             columns=['Feature importance']).sort_values(by=\"Feature importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this case, the accuracy doesn't change a lot, since the speed is so much more important than any other features. \n",
    "\n",
    "We can nonetheless observe that the type advantage is the third most important feature, whereas in the original encoding, the information was basically useless to the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now we can predict new battle outcomes\n",
    "\n",
    "We've been interating on our test data with different algorithms and models and hyperparameters, but what if we learned how to be good on this data, and not on real life data (overfitting) ?\n",
    "There are data in another dataset that we've left untouched, to simulate real life data that we really can't predict. Let's see how we perform on those.\n",
    "\n",
    "We will need to perform the same preprocessing steps that we ran on the training data to prepare the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify(type_series, prefix):\n",
    "    # we need to give our classifier data with the same columns as the data it has learned on\n",
    "    # in particular, we need to have all type columns present, even if they are not in the test set\n",
    "    # that's why we extract all the types from the trainset, and fill the missing columns with zeros\n",
    "    type_columns = sorted(prefix + '_' + tc for tc in pokemons['Type 1'].unique())\n",
    "    dummies = pd.get_dummies(type_series.iloc[:,0], prefix=prefix).add(\n",
    "        pd.get_dummies(type_series.iloc[:,1], prefix=prefix),\n",
    "        fill_value=0)\n",
    "    missing_cols = set(type_columns) - set( dummies.columns )\n",
    "    for c in missing_cols:\n",
    "        dummies[c] = 0\n",
    "    return dummies[type_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(battles_test):\n",
    "    # we need to apply all the preprocessing steps we applied on our training data\n",
    "    # 1°) joining data about battle outcomes and pokemons\n",
    "    df_test = battles_test.join(pokemons, on='First_pokemon').join(pokemons, on='Second_pokemon', rsuffix='_opponent')\n",
    "    # 2°) encoding the winner as int (1 if First_pokemon wins, 0 else)\n",
    "    if 'Winner' in df_test.columns:\n",
    "        df_test['label'] = (df_test['First_pokemon'] == df_test['Winner']).astype(int)\n",
    "        df_test = df_test.drop(columns=['Winner'])\n",
    "    # 3°) dropping useless columns\n",
    "    df_test = df_test.drop(['First_pokemon', 'Second_pokemon', \n",
    "                               'Name', 'Name_opponent', \n",
    "                               'Generation', 'Generation_opponent'], axis=1)\n",
    "    # 4°) encoding boleans as int\n",
    "    df_test['Legendary'] = df_test['Legendary'].astype(int)\n",
    "    df_test['Legendary_opponent'] = df_test['Legendary_opponent'].astype(int)\n",
    "    # 5°) one-hot encoding of categorical columns \n",
    "    types = dummify(df_test[['Type 1', 'Type 2']], prefix='Type')\n",
    "    types_opponents = dummify(df_test[['Type 1_opponent', 'Type 2_opponent']], prefix='Opponent_Type')\n",
    "    return pd.concat((df_test, types, types_opponents), axis=1).drop(['Type 1', 'Type 2', 'Type 1_opponent', 'Type 2_opponent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "battles_test = pd.read_csv('pokemon-challenge/battles_test.csv')\n",
    "df_ml_test = preprocess_test_data(battles_test)\n",
    "df_ml_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = df_ml.drop('label', axis=1).as_matrix()\n",
    "y = df_ml['label'].as_matrix()\n",
    "X_test = df_ml_test.drop('label', axis=1).as_matrix()\n",
    "y_test = df_ml_test['label'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=40)\n",
    "rf.fit(X, y)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our classifiers are trained, and that they seem to perform well enough, we can write a small function to predict which Pokemon will win a battle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_battle_outcome(pokemon1, pokemon2, trained_estimator):\n",
    "    if isinstance(pokemon1, str):\n",
    "        try:\n",
    "            pokemon1 = pokemons.index[pokemons['Name'] == pokemon1][0]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"The name {pokemon1} is not a valid Pokemon name\")\n",
    "    \n",
    "    if isinstance(pokemon2, str):\n",
    "        try:\n",
    "            pokemon2 = pokemons.index[pokemons['Name'] == pokemon2][0]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"The name {pokemon2} is not a valid Pokemon name\")\n",
    "    df = pd.DataFrame(data=[[pokemon1, pokemon2]], columns=['First_pokemon', 'Second_pokemon'])\n",
    "    X = preprocess_test_data(df).as_matrix()\n",
    "                \n",
    "    if trained_estimator.predict(X)[0]:\n",
    "        print(f\"{pokemons.loc[pokemon1, 'Name']} wins\")\n",
    "    else:\n",
    "        print(f\"{pokemons.loc[pokemon2, 'Name']} wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_battle_outcome('Charizard', 'Mega Venusaur', rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dev]",
   "language": "python",
   "name": "conda-env-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
